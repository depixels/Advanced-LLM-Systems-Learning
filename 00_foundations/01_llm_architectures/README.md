# 01: LLM Architectures

This section explores the architectural foundations of Large Language Models (LLMs). We begin with the seminal Transformer architecture, dissecting its core components, and then move towards understanding its variants, scaling properties, analysis methods, and key optimizations.

## Table of Contents

### 1. Transformer Fundamentals

The foundational concepts of the Transformer model, which revolutionized sequence modeling.

*   [**Attention Mechanism**](./1_transformer_fundamentals/attention_mechanism.md): Understanding the core Self-Attention, Multi-Head Attention, and Cross-Attention mechanisms.
*   [**Feed-Forward Networks**](./1_transformer_fundamentals/feed_forward_networks.md): Exploring the position-wise feed-forward layers within the Transformer block.
*   [**Layer Normalization**](./1_transformer_fundamentals/layer_normalization.md): How normalization is applied to stabilize training and improve performance.
*   [**Positional Encoding**](./1_transformer_fundamentals/positional_encoding.md): Techniques used to inject sequence order information into the model.

### 2. Transformer Variants

*(Content to be added)* - Exploring architectures derived from or inspired by the original Transformer (e.g., BERT, GPT variants, Transformer-XL).

### 3. Scaling Laws

*(Content to be added)* - Investigating the relationship between model size, dataset size, compute, and performance.

### 4. Model Analysis

*(Content to be added)* - Techniques for understanding and interpreting the behavior and capabilities of LLMs.

### 5. Attention Optimizations

*(Content to be added)* - Methods to make the attention mechanism more computationally efficient (e.g., Sparse Attention, Linear Attention).

---

Navigate through these topics to understand the building blocks and evolution of modern LLM architectures.

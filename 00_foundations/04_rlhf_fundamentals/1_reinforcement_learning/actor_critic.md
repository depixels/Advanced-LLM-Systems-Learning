# 演员-评论家 (Actor-Critic): 策略与价值的完美二人转

> 在前面的探索中，我们走过了两条截然不同的道路：
> 1.  **基于价值 (Value-Based)**：学习一张精确的“价值地图”（Q函数），但难以应对连续动作空间。
> 2.  **基于策略 (Policy-Based)**：直接学习一个“行动策略”（策略网络），但像REINFORCE这样的基础算法，因为使用了嘈杂的总回报 `Gt` 作为学习信号，导致训练过程方差巨大，极不稳定。
>
> 我们在[《策略梯度》](./policy_gradient.md)的结尾处发现，通过引入一个基线 `V(s)` 来计算优势函数 `A(s,a) = Gt - V(s)`，可以有效降低方差。
>
> 这个发现，正是通往强化学习中最强大、最流行的框架——**演员-评论家 (Actor-Critic)** 的大门。它不再是非此即彼的选择，而是将两者巧妙地结合在一起，互相促进，共同进化。

---

## 核心思想：一个行动，一个评价

Actor-Critic 方法将智能体一分为二，变成了两个协同工作的神经网络：

1.  **演员 (The Actor)**
    -   **角色**: 策略网络 `π(a|s; θ)`。
    -   **任务**: 负责思考和决策。它观察当前状态 `s`，然后决定要**采取什么动作 `a`**。它就是我们之前讨论的“基于策略”的部分。

2.  **评论家 (The Critic)**
    -   **角色**: 价值网络 `V(s; w)` 或 `Q(s, a; w)`。
    -   **任务**: 负责评价和打分。它观察演员的决策，然后**评价这个决策有多好**。它就是我们之前讨论的“基于价值”的部分。

**这个组合如何解决高方差问题？**

关键在于，演员不再需要等到一整个回合结束，去接收那个充满噪声的、延迟的最终总回报 `Gt`。取而代之的是，它能从评论家那里得到一个**即时的、更稳定的指导信号**。

![Actor-Critic Diagram](https://storage.googleapis.com/agent-tools-public-hub-prod/medias/A-Z_so/actor_critic_diagram.png)

## 学习流程：精妙的协同工作

让我们来分解一下这个“二人转”是如何配合的：

1.  **行动 (Actor)**: 在状态 `St`，演员根据其当前的策略 `π(a|s; θ)`，选择并执行一个动作 `At`。
2.  **互动**: 环境收到动作 `At`，给出一个即时奖励 `Rt+1` 和下一个状态 `St+1`。
3.  **评价 (Critic)**: 评论家观察到了这次转移 `(St, At, Rt+1, St+1)`。现在，它要计算出这次行动的“价值”所在。它通过计算**时序差分误差 (Temporal Difference Error, TD Error)** `δt` 来完成这一点。
    -   首先，评论家计算一个**TD目标**，这是它对“真实”价值的估计：
        \[Y_t = R_{t+1} + \gamma V(S_{t+1}; w)\]
        (即时奖励 + 下一个状态的折扣价值)
    -   然后，它计算**TD误差** `δt`，这可以被看作是**优势函数 `A(s,a)` 的一个很好的估计**：
        \[\delta_t = Y_t - V(S_t; w) = R_{t+1} + \gamma V(S_{t+1}; w) - V(S_t; w)\]
        这个误差衡量了“实际发生的情况”（TD目标）与“评论家之前的预期” `V(St)` 之间的差距。
4.  **共同学习 (Update)**: 这个TD误差 `δt` 现在成为了一个强大的学习信号，同时指导演员和评论家进行更新。
    -   **评论家更新**: 评论家希望自己的预测越来越准，也就是让TD误差越来越小。所以它根据 `δt` 来更新自己的参数 `w`。（例如，通过梯度下降最小化 `δt²`）。
        \[w \leftarrow w + \beta \cdot \delta_t \cdot \nabla_w V(S_t; w)\]
    -   **演员更新**: 演员使用 `δt` 作为策略梯度的“分数”，来代替原来REINFORCE算法中那个嘈杂的 `Gt`。
        \[\theta \leftarrow \theta + \alpha \cdot \delta_t \cdot \nabla_{\theta} \log \pi_{\theta}(A_t|S_t)\]

**这个更新过程的直觉是：**
-   如果 `δt > 0` (一个正向的惊喜)，说明动作 `At` 的结果比评论家预期的要好。因此，演员会调整参数 `θ`，以增加未来在 `St` 状态下选择 `At` 的概率。
-   如果 `δt < 0` (一个负向的意外)，说明动作 `At` 的结果比预期的要差。演员就会调整参数，降低选择这个动作的概率。

## Actor-Critic为何如此强大？

1.  **低方差**: TD误差 `δt` 只依赖于一步的真实奖励 `Rt+1` 和评论家的价值估计，相比于依赖一整个回合所有奖励的 `Gt`，它的随机性要小得多，因此学习信号的“信噪比”更高，训练过程更稳定。
2.  **在线学习与效率**: 智能体每走一步都可以进行一次学习和更新，而不需要像REINFORCE那样必须等待一个回合结束。这使得学习更加及时和高效。

## 总结：通往现代强化学习的基石

Actor-Critic 框架是一个里程碑式的进步。它优雅地融合了Value-Based和Policy-Based方法的优点：
-   它像**Policy-Based**方法一样，可以直接优化策略，轻松处理连续动作空间。
-   它又借鉴了**Value-Based**方法的思想，利用价值函数（评论家）来生成低方差的学习信号，克服了纯策略梯度方法不稳定的核心弊病。

这个“演员+评论家”的结构是如此强大和灵活，以至于它成为了当今许多最先进算法的基石，包括A2C/A3C、DDPG、SAC等等。

当然，也包括我们这个系列的最终目标——**PPO**。PPO可以被看作是Actor-Critic家族中的一个“优等生”，它在Actor的更新环节增加了一个巧妙的“信任区域”约束，使得学习过程在保持高效的同时，变得前所未有的稳定。

现在，我们已经铺平了所有道路，下一站，就是终点：PPO。

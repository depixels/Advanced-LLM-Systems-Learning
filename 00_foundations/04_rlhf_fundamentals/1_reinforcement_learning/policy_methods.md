# RL算法三大流派：我该如何找到最优策略？

> 在前面的文章中，我们已经打下了坚实的基础：
> 1.  [**MDP**](./mdp_framework.md) 帮助我们用数学语言**描述**问题。
> 2.  [**价值函数**](./value_functions.md) 给了我们一把标尺，用以**评估**一个策略有多好。
>
> 现在，我们终于来到了所有强化学习研究者最关心的问题：**我们到底该如何找到那个能获得最高分的最优策略 `π*` 呢？**
>
> 就像武侠世界里有剑宗和气宗之分，强化学习领域也发展出了几种不同的“修炼法门”。这些方法在如何找到最优策略上思路迥异，最终形成了三大主流方法：**基于价值 (Value-Based)**、**基于策略 (Policy-Based)**，以及集两者之长的**演员-评论家 (Actor-Critic)**。

---

## 1. 基于价值的方法 (Value-Based)

这是最直观的一种思路。既然我们知道最优Q函数 `Q*(s, a)` 能直接导出最优策略，那我们的目标就干脆定为：**学习这个Q函数**！

-   **核心思想**: 不直接去学策略，而是学习一个精确的“价值地图”或“打分表”（即Q函数）。策略是这个地图的副产品，是隐式的。
-   **工作方式**: 智能体维护一个表格（Q-Table）或者一个函数（如神经网络），用来估算每一个“状态-动作”对的价值。在决策时，它会查看当前状态下所有动作的Q值，然后贪心地选择那个Q值最高的动作。
    $\text{Action} = \arg\max_{a} Q(s, a)$
-   **生活类比**: **“美食评论家”**。
    这位评论家想在城市里找到最佳的用餐路线。他的方法不是为自己规划一条固定的“美食路线”（策略），而是给全城的每一家餐厅的每一道菜（状态-动作对）都打一个精确的分数（Q值）。到了任何一个路口（状态），他只需要拿出他的评分手册，看看附近哪家餐厅的哪道菜分最高，然后就去那家。
-   **代表算法**:
    -   **Q-Learning**: 使用表格来存储Q值的经典算法。
    -   **Deep Q-Networks (DQN)**: 使用深度神经网络来近似Q函数，成功地玩转了雅达利游戏，是深度强化学习的里程碑。
-   **适用场景**:
    -   **优点**: 在低维、离散的动作空间中通常非常稳定和高效。
    -   **缺点**: 很难处理连续动作空间。想象一下，你无法为机器人手臂转动的“无数种”精确角度都创建一个条目来打分。

## 2. 基于策略的方法 (Policy-Based)

这类方法认为，既然最终目标是策略，为什么不直接一点，**直接学习策略本身**呢？

-   **核心思想**: 将策略参数化，通常用一个神经网络 `π(a|s; θ)` 来表示。我们的目标是找到最好的参数 `θ`，使得策略能够获得最大的回报。
-   **工作方式**: 智能体不学习价值。它根据当前策略 `πθ` 执行动作，完成一个或多个回合的试验，然后根据这些试验结果的好坏（总回报是高是低），回过头来调整策略网络的参数 `θ`。这个调整的方向由**策略梯度 (Policy Gradient)** 决定。
-   **生活类比**: **“肌肉记忆的运动员”**。
    一个篮球运动员练习投篮。他不会去计算手臂每个角度、每个力度对应的“得分期望值”（Q值）。相反，他直接一次又一次地投篮（执行策略），如果球进了（高回报），他就会强化刚才这一整套动作模式（增加策略参数`θ`在那个方向的权重）；如果没进，就稍微调整动作。他直接优化的是“投篮”这个行为本身。
-   **代表算法**:
    -   **REINFORCE**: 最基础的策略梯度算法。
-   **适用场景**:
    -   **优点**: 非常适合连续或高维动作空间。可以直接学习随机策略，这对于探索和某些特定问题非常重要。
    -   **缺点**: 训练过程可能非常不稳定，容易陷入局部最优。因为单次回合的奖励波动很大，导致梯度估计的方差很高（即所谓的“学习信号”噪声很大）。

## 3. 演员-评论家方法 (Actor-Critic)

既然Value-Based和Policy-Based各有优劣，我们能不能把它们结合起来，取长补短呢？于是，Actor-Critic方法应运而生。

-   **核心思想**: **同时学习策略和价值函数**。系统包含两个部分，它们互相学习，共同进步。
    -   **演员 (Actor)**: 就是一个策略网络，负责根据当前状态**选择动作**。它就是我们上面提到的“基于策略的”部分。
    -   **评论家 (Critic)**: 就是一个价值网络，负责**评估**演员选择的动作有多好。它学习状态价值 `V(s)` 或动作价值 `Q(s, a)`，为演员提供更稳定、更低方差的指导信号。
-   **工作方式**: 演员根据当前策略做出一个动作。评论家观察这个动作，并给出评价：“你这个动作比我预期的要好（或差）”。演员根据评论家的这个反馈（而不是像REINFORCE那样等待整个回合结束后的总回报）来更新自己的策略。
-   **生活类比**: **“演员和他的专属导演”**。
    一个演员（Actor）在舞台上表演。他根据自己的理解（策略）说出台词。旁边的导演（Critic）立刻给出反馈：“你这句台词情绪饱满，比剧本的平均水平要好！（正向反馈）” 或者 “太平淡了，没有表现出角色的内心挣扎！（负向反馈）”。演员根据导演的即时反馈，马上调整下一句台词的表演方式。
-   **代表算法**:
    -   **A2C / A3C**: 同步/异步优势演员-评论家算法。
    -   **DDPG**: 用于连续控制的深度确定性策略梯度。
    -   **PPO (Proximal Policy Optimization)**: 我们这个系列最终的目标！它是一种先进的Actor-Critic算法，因其出色的稳定性和性能而广受欢迎。
-   **适用场景**:
    -   **优点**: 结合了两者的优势。训练比纯策略梯度方法更稳定（因为Critic提供了更好的指导信号），同时也能处理复杂的连续动作空间。目前许多复杂任务的最优解都基于Actor-Critic框架。

## 总结与展望

| 方法           | 学习目标             | 策略产生方式     | 优点                           | 缺点                       |
| :------------- | :------------------- | :--------------- | :----------------------------- | :------------------------- |
| **Value-Based**  | 学习价值函数 `Q(s,a)` | 隐式（贪心选择） | 稳定，样本效率高               | 难以处理连续动作         |
| **Policy-Based** | 直接学习策略 `π(a\|s)` | 显式（直接输出） | 适用于连续动作，可学随机策略 | 训练不稳定，方差高         |
| **Actor-Critic** | 同时学习策略和价值 | 显式 + 内部指导  | 稳定且适用于连续动作，性能强大 | 结构更复杂，需要调优两个网络 |

通过这个高层视角，我们能清晰地看到强化学习算法的演进脉络。为了解决更复杂、更现实的问题，研究者们从简单的价值和策略方法，逐步走向了更强大、更稳定的Actor-Critic框架。

我们接下来的旅程，将深入这个框架的内部。首先，我们将解构“演员”的大脑——**策略梯度 (Policy Gradient)**，它是驱动策略网络学习的根本动力。

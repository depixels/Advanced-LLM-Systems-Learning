# 策略梯度：如何让策略“自我进化”？

> 在上一篇[《RL算法三大流派》](./policy_methods.md)中，我们认识了直接优化策略的“基于策略的方法”。我们用“肌肉记忆的运动员”来比喻它：通过反复尝试并根据结果的好坏来直接调整动作本身。
>
> 这种“调整”并非盲目。它背后有坚实的数学理论支持，这个理论的核心就是**策略梯度 (Policy Gradient)**。策略梯度是驱动策略网络进行学习和进化的根本动力。
>
> 今天，我们就来揭开这个“魔法”背后的秘密。

---

## 核心问题：如何衡量策略的好坏？

首先，我们需要一个明确的目标。对于一个由参数 `θ` 决定的策略 `πθ`，我们如何用一个数值来评价它的好坏？答案是定义一个**目标函数 `J(θ)`**，它代表了遵循该策略所能获得的期望总回报。

$J(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}}[R(\tau)]$

-   **解读**:
    -   `τ` 代表一个完整的轨迹（episode）: `(s0, a0, r1, s1, a1, r2, ...)`。
    -   `R(τ)` 是这条轨迹的总奖励。
    -   `Eτ~πθ[...]` 的意思是，我们按照策略 `πθ` 去玩游戏，会产生很多很多可能的轨迹，我们想要的是这些轨迹的平均总奖励。

我们的目标就是找到一组最好的参数 `θ`，来**最大化这个目标函数 `J(θ)`**。

## 梯度上升：朝着“更好”的方向前进

在机器学习中，如果我们想最小化一个损失函数，我们会使用**梯度下降**。现在，我们想最大化一个目标函数，所以我们用**梯度上升**。

梯度上升的更新规则非常简单：
$\theta_{new} = \theta_{old} + \alpha \nabla_{\theta}J(\theta)$

-   `α` 是学习率，控制我们每一步走多大。
-   `∇θJ(θ)` 是目标函数 `J(θ)` 对策略参数 `θ` 的**梯度**。这个梯度指向了能让 `J(θ)` 增长最快的方向。

现在，所有问题都归结为一点：**如何计算这个梯度 `∇θJ(θ)`？**

这很棘手。因为奖励 `R(τ)` 不仅取决于我们的策略 `πθ`，还取决于环境的状态转移 `P(s'|s,a)`，而环境是不可微分的，我们无法直接对它求导。

## 策略梯度定理 (The Policy Gradient Theorem)

幸运的是，数学家们为我们提供了一个神奇的捷径，它绕过了对环境求导的难题，这就是**策略梯度定理**。它证明了，目标函数的梯度可以被写成一个只和策略 `πθ` 相关的期望形式：

$\nabla_{\theta}J(\theta) = \mathbb{E}_{\pi_{\theta}} \left[ G_t \nabla_{\theta} \log \pi_{\theta}(A_t|S_t) \right]$

这个公式是现代强化学习的基石之一，让我们把它拆解开来，看看它到底在说什么：

1.  **`πθ(At|St)`**: 这是我们的策略网络。输入当前状态 `St`，输出我们实际采取的动作 `At` 的概率。
2.  **`log πθ(At|St)`**: 对这个概率取对数。取对数是一种常见的数学技巧，它将乘法变为加法，并且在数值上更稳定。
3.  **`∇θ log πθ(At|St)`**: **这是最关键的部分**。这是`log π`对参数`θ`的梯度。
    *   它的**方向**告诉我们：应该如何调整参数`θ`，才能让刚才我们采取的动作 `At` 的概率**变得更高**。
    *   这个计算完全不涉及环境，只和我们自己的策略网络有关，因此是完全可微的。
4.  **`Gt`**: 这是在时间 `t` 之后我们获得的总回报 (Return)。
    *   它是一个**标量**，充当了上面那个梯度的**权重**或**“分数”**。
    *   如果 `Gt` 是一个大的正数（我们这步之后表现很好），它就会让 `θ` 朝着 `∇θ log π` 的方向大幅更新，从而增加未来在 `St` 状态下选择 `At` 的可能性。
    *   如果 `Gt` 是一个负数（我们这步之后表现很差），它就会让 `θ` 朝着 `∇θ log π` 的**相反方向**更新，从而降低未来选择 `At` 的可能性。

**一言以蔽之，策略梯度定理告诉我们：增加“好”动作的概率，降低“坏”动作的概率。**

## REINFORCE算法：策略梯度的直接应用

最直接地应用策略梯度定理的算法叫做 **REINFORCE**。它的流程非常朴素：

1.  用当前的策略 `πθ` 玩一整个回合的游戏，并记录下整个轨迹 `(s0, a0, r1, s1, ...)`。
2.  对于轨迹中的每一步 `t`，计算从这一步开始的总回报 `Gt`。
3.  计算每一步的策略梯度项 `Gt * ∇θ log πθ(At|St)`。
4.  将所有步的梯度加起来，然后用梯度上升更新参数 `θ`。
5.  重复以上过程。

## 策略梯度的致命弱点：高方差

REINFORCE 算法虽然理论优美，但在实践中却有一个巨大的问题：**训练极其不稳定**，也就是**高方差 (High Variance)**。

**为什么？** 因为 `Gt` 这个“分数”太嘈杂了。

想象一下，在一局游戏中，智能体在前期做出了很多非常明智的决策，但在最后一步犯了个愚蠢的错误导致游戏失败，获得了很低的总回报 `G`。

根据 REINFORCE 的规则，这个低的 `G` 会被应用到**所有**的决策上，导致那些前期的英明决策也被“惩罚”了。这显然是不公平的，也极大地干扰了学习过程。这种“信噪比”低的问题，就是高方差的根源。

## 解决方案：引入基线 (Baseline)

如何让我们的“打分”机制更公平？一个聪明的想法是：**不要只看绝对分数 `Gt`，而是看它比平均水平好多少。**

我们引入一个**基线 (Baseline)** `b(St)`，它通常是状态价值函数 `V(St)` 的一个估计。然后，我们用 `Gt - b(St)` 来代替原来的 `Gt`。

新的梯度更新公式变为：
$\nabla_{\theta}J(\theta) = \mathbb{E}_{\pi_{\theta}} \left[ (G_t - V(S_t)) \nabla_{\theta} \log \pi_{\theta}(A_t|S_t) \right]$

这个 `Gt - V(St)` 有一个专门的名字，叫做**优势函数 (Advantage Function) `A(s, a)`**。它衡量了在状态 `s` 下，采取动作 `a` 相比于平均水平有多大的“优势”。

-   如果 `A > 0`，说明这个动作比预期的要好，值得鼓励。
-   如果 `A < 0`，说明这个动作比预期的要差，需要抑制。

通过引入基线，我们显著降低了梯度的方差，使得学习过程更加稳定和高效。

## 总结与展望

-   **策略梯度**是一种强大的工具，它让我们能够直接通过梯度上升来优化策略网络。
-   其核心思想是**增加高回报动作的概率，降低低回报动作的概率**。
-   纯粹的策略梯度方法（如REINFORCE）因为**高方差**问题而难以训练。
-   引入**基线**（价值函数）来计算**优势函数**，是解决高方差问题的关键。

这个“引入基线”的思想，自然而然地把我们引向了强化学习的第三大流派，也是目前最成功的一派：**演员-评论家 (Actor-Critic)**。

在下一篇文章中，我们将看到“演员”（策略）和“评论家”（价值函数）是如何协同工作，从而实现稳定、高效的学习的。

# 04: RLHF Fundamentals

This section explores the fundamentals of Reinforcement Learning from Human Feedback (RLHF), a critical paradigm for aligning large language models with human values and intentions.

## Table of Contents

### 1. Reinforcement Learning

Core concepts and algorithms from reinforcement learning that form the foundation of RLHF.

*   [**MDP Framework**](./1_reinforcement_learning/mdp_framework.md): Introduction to Markov Decision Processes, the mathematical framework for modeling decision-making.
*   [**Value Functions**](./1_reinforcement_learning/value_functions.md): Understanding state-value and action-value functions which are central to many RL algorithms.
*   [**Policy Methods**](./1_reinforcement_learning/policy_methods.md): An overview of different policy-based reinforcement learning methods.
*   [**Policy Gradient**](./1_reinforcement_learning/policy_gradient.md): Methods for directly optimizing the policy function.
*   [**Actor-Critic**](./1_reinforcement_learning/actor_critic.md): Combining value-based and policy-based methods for more stable and efficient learning.
*   [**PPO Algorithm**](./1_reinforcement_learning/ppo_algorithm.md): Deep dive into Proximal Policy Optimization, a key algorithm used in the RL phase of RLHF.
*   [**Exploration vs. Exploitation**](./1_reinforcement_learning/exploration_exploitation.md): The fundamental tradeoff in reinforcement learning.

### 2. Human Feedback

*(Content to be added)* - How human preferences are collected and modeled to create a reward signal.

### 3. RLHF Pipeline

*(Content to be added)* - The end-to-end process of RLHF, from supervised fine-tuning to reward modeling and RL optimization.

### 4. Alignment Objectives

*(Content to be added)* - Exploring the goals of alignment, such as helpfulness, honesty, and harmlessness.

### 5. Alternative Approaches

*(Content to be added)* - Other methods for model alignment beyond the standard RLHF pipeline.

---

Navigate through these topics to build a comprehensive understanding of RLHF.
